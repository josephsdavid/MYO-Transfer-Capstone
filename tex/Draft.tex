%Note that subject splitting may not be of utmost exigence until we can fit within subjects.

% This is samplepaper.tex, a sample paper demonstrating the
% LLNCS class package for the SMU Data Science Review Journal;
%
% This sample paper is a modified version of samplepaper.tex for
% the Springer Computer Science proceedings; Version 2.20 of 2017/10/04
%
% Version 1.0 2019/06/03

% Use the llncs.cls formatting
\documentclass{llncs}

% Set the packages for use within the document. The following 
% packages should be included.  Additional packages that do not
% conflict with these packages or change the llncs class formatting
% may be used.  Packages that do change the formatting are
% not allowed.
\usepackage{graphicx} % Used for displaying a sample figure. 
% If possible, figure files should be included in EPS format. 
% PDF format is also acceptable. JPEG  will work, but some of 
% them are downsampled resulting in fuzzy images.
\usepackage{booktabs} % Better horizontal rules in tables
\usepackage{multirow} % Better combined rows in tables
\usepackage{amsmath}
\usepackage{array}
\usepackage{hyperref}

% The title of the paper
% This is shite
\title{sEMG Gesture Recognition With a Simple Model of Attention}

% The complete list of authors with their affiliations
\author{%
David Josephs\inst{1} \and
Carson Drake\inst{1} \and
Che' Cobb\inst{1} \and
John Santerre\inst{1} \and
Jennifer Graves %triple verify graves still part of the team
}

% The Institutes and emails associated with each author. All students
% should use their MSDS affiliation or a generic SMU affiliation.
% Advisors should use their appropriate affiliation. Note that advisors
% are NOT referenced or otherwise denoted as advisors. Advisors
% are simply co-authors on the paper.
% Note that the emails for the MSDS affiliation, show how 
% to list emails that have the same organization portion.
\institute{%
Master of Science in Data Science, Southern Methodist University,
Dallas TX 75275 USA 
\email{\{josephsd, drakec, cobbc\}@smu.edu} %\and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com} \\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}
}
% Begin the document
\begin{document}

\maketitle              % typeset the title and author of the paper

% Reset the footnote counter
\setcounter{footnote}{0}
% The abstract environment uses the \begin{} and \end{} constructs to 
% denote the beginning and ending of the abstract contents.
\begin{abstract} 
	Idk man
%In this paper, we present a set of standard procedures for training and evaluating deep learning models for EMG classification. We evaluate the most popular and established preprocessing and augmentation procedures. Standards for window sizing, data partitioning, and %evaluation dataset
%model evaluation are proposed. These standards are designed for reproducibility, generalizability (source model), bootstrapping (specialization), and transfer learning. We also highlight a novel recurrent architecture, and demonstrate its utility for live gesture prediction, with and without transfer learning.
%%* training procedures 
%    * preproc
%    * aug
%    * partitioning (good and bad procedures)
%    * window sizing
%    * transfer learning procedures
%* evaluation procedures
%    * intra AND inter subject evaluation
%    * Choice of dataset: 
%        * all 3 subsets of nina5 as well as all of nina5
%We also present a novel recurrent architecture for sEMG classification and demonstrate its utility for live gesture prediction, with and without transfer learning.
%
%* We present standard methods for training and evaluating deep learnign models for the purpose of low frequence sEMG classification
%
%* We also present a novel LSTM architecture (with and without TL) FOR ACTUAL USE IN PROSTHETIC LIMBS
%
%* Explain WHAT WE USE AND HOW WE USED IT
%
%* CONCLUSION PLACEHOLDER

% Problem statement


% Broad motivation

% Keywords may be used, but they are not required.
%\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}

% Sections are denoted by the use of the \section{Section Name} 
% command -- where "Section Name" is the name you give to the Section.
\section{Introduction}
Electrophysiological studies of the nervous system are the core area of research in clinical neurophysiology, where scientists attempt to link electrical signals from the body to real world effects. These studies include measuring brain waves (electroencephalography), comparison of sensory stimuli to electrical signals in the central nervous system (evoked potential), and the measure of electrical signals in skeletal muscles (electromyography). 
Electromyography is of particular interest to this paper. The nervous system uses electrical signals to communicate with the rest of the body. When a signal from the nervous system reaches a skeletal muscle, the myocites (muscle cells) contract, causing a physical motion. By measuring these electrical signals in a supervised manner, we can develop a link between signal and physical action. This connection yields many powerful uses, ranging from quantifying physical veracity to diagnosing neurodegenerative diseases. An example of the latter can be found in Akhmadeev et al. \cite{graves}, where electromyographic (EMG) signals were used to classify Multiple Sclerosis patients from healthy control subjects with 82\% accuracy.

Deep learning can be used to further improve the power and utility of the EMG analysis. A deep neural network is, in essence, a composition of neurons (regressors) that learns a functional mapping between two sets of data. By learning a mapping between EMG signal and physical effect, we can develop more sensitive and accurate models of what connects the two. This also allows us to use less intrusive measurement devices in studies and in the real world. The applications of this range from clinical trials and prognostication of neuromuscular diseases to gesture prediction in "brain-controlled" prosthetic limbs.

%Although deep learning is a powerful tool which can revolutionize the field of EMG study, there are several issues with the current state of research. First, the results of the latest research are difficult to compare and can be misleading, due to a lack of standards for model evaluation. Models are often evaluated on different subsets of the same dataset, making comparison impossible.  The results are also often misleading due to significant overfitting of the test subjects \cite{systematic}. 

% DITCH LATENCY
% RESTRUCTURE INTRO

There are several factors which need to be considered in order to develop practically useful deep learning models for real time gesture prediction (myoelectric control)\footnote{Myoelectric control in this context means classifying a gesture well before it is completed, using myoelectrographic signals. These predictions need to be done fast enough for the user to feel that the gesture is being made as they decide to complete it.}. Arguably the most important of these is the amount of time before the model makes its first prediction (referred to as "prediction latency"). 
The absolute largest prediction latency for a model to be considered useful for myoelectric control lies between 250 and 300 milliseconds  \cite{300ms}, \cite{250ms}. Many recent papers claim excellent gesture prediction results, however they require time samples (windows) between 0.5 and 1.5 seconds long \cite{rnn_1000}, \cite{rnn_128}. In this paper, all windows are 260 milliseconds in length, following the precedent set by Allard et al. \cite{primary}. This leaves 40 milliseconds for any transformations done on the window, as well as the amount of time it takes for the model to make a prediction.


The next factor to consider when designing a model for myoelectric control is the number of gestures the model is actually capable of predicting. In many recent papers, models are evaluated using between six and eight gestures. This presents an interesting theoretical result, but does not hold much practical value. Thus, we evaluate our model on 53 gestures within 3 subclasses: 12 fine finger movements, 17 wrist movements, 23 functional movements (such as grasping), and rest (present in all subclasses). 

It is also important to account for data collection. As the intention of this research is to provide a practical myoelectric control tool for mass use in real hardware (prosthetic limbs, robotic arms, etc.), we elected to use a cheap sEMG sensor, the Thalmic MYO armband (referred to as "MYO armband") \cite{myo}. The MYO armband collects data at a relatively low frequency, however it costs about \$100 and has several other qualities which make it an ideal tool for this research. The most important of these qualities is standardization. In clinical sEMG research, sensors are placed on a per patient basis in exact locations, determined by medical professionals. This is not feasible in the case of prosthetics, as requiring a doctor be present to put your hand on every day would be far from convenient. In contrast, the MYO armband automatically contracts to more or less the same points on every arm. This makes it far more practical, and it far simpler to imply the model will generalize to more subjects.

The final two factors to consider with respect to model and experimental design are related to the evaluation and generalization of the model. A myoelectric control model needs to bear two qualities: robustness over time and generalizability from subject to subject. Over time, the armband will shift, the user will become fatigued, and actions will change. For a model to be useful for mass production in prosthetics and robotic arms, it will also need to work on new people. While it is possible to train a model on a new person, even on amputees, it is preferable for the model to work immediately \cite{amputeedb}.


% This is NEW! CHANGE ABOVE!^^6
In this paper, we propose a novel attentional architecture for sEMG recognition and myoelectric control. We demonstrate the model's validity on the 53-class NinaPro DB5 \cite{nina5}. We also compare different techniques  for dealing with the inherent class imbalance in sEMG, a synthetic data based approach (augmentation), an undersampling based approach, and ,w

We also present and discuss several methods for dealing with the class imWith this, we also present a procedure for dealing with class imbalance in myoelectric data, a novel data augmentation technique, and a variety of preprocessing techniques. We also propose the usage of the 5th NinaPro MYO database as a standard benchmark dataset for myoelectric control \cite{nina5}. A common benchmark dataset will help current results be easily comparable, which is a large issue facing myoelectric control today. Our evaluation procedures will also help evaluate the issues brought up in \cite{systematic}, in which practical results do not match theoretical results due to overfitting.  %Although deep learning is a powerful tool which can revolutionize the field of EMG study, there are several issues with the current state of research. First, the results of the latest research are difficult to compare and can be misleading, due to a lack of standards for model evaluation. Models are often evaluated on different subsets of the same dataset, making comparison impossible.  The results are also often misleading due to significant overfitting of the test subjects \cite{systematic}.

%At a given window size an
%
%Model cheapness (MYO)
%
%Number of gestures
%
%Functionality over time (out of rep)
%
%Generalization (out of subject)
%
%One factor which yields excellent theoretical results but little practical value is improper model and training parameters. A major offender in this regard is improper selection of sequence length for classification. Smaller EMG sequences are harder to correctly classify and easier to overfit. The use for many EMG classifiers in recent research is gesture recognition and prediction in embedded hardware. For this purpose, the absolute limit of time sequence length is between 250 and 300 milliseconds  \cite{300ms}, \cite{250ms}. However, many recent papers, such as \cite{rnn_1000} and \cite{rnn_128}, present networks which need input sequences three to five times longer than this maximum. These models are accurate, but not useful in practice. Other research focuses on expensive high-frequency sensors, which are both less accessible and less standardized.
%
%Deep learning requires a massive amount of training data. This presents a significant challenge in the EMG space, due to lack of test subjects \cite{systematic}. Therefore,  augmenting training data is a best practice. Despite this, there is no comprehensive evaluation of EMG data augmentation practices. Similarly, there exist a wide array of data preprocessing methods for EMG signals, which are not yet standardized.
%
%
%In this paper, we will address these issues as follows. First, we will comprehensively evaluate current "best practice" preprocessing procedures on an array of standard models: a convolutional neural net (CNN), a recurrent neural net (RNN), a convolutional recurrent neural net (CRNN), and a standard feed forward deep neural network (DNN). We will evaluate data augmentation techniques in a similar manner. To  address the lack of repeatability and comparability of models, we propose a new evaluation procedure and standardize the evaluation dataset. All models will be trained and evaluated both on an intra-subject basis and an inter-subject basis. Intra-subject evaluation demonstrates that the model is robust to elements such as muscle fatigue. In contrast, inter-subject evaluation demonstrates that a model is capable of generalizing to new people \cite{ring_2018}. We also propose a standard evaluation set for low frequency EMG classification: the NinaPro5 database \cite{nina5}. The NinaPro5 database contains 52 gestures collected 6 times each at 200 Hz by 10 subject, split into three subsets of 12, 17, and 23 classes. We propose to evaluate models on each subset separately and then on all subsets combined, with both intra-subject and inter-subject evaluation.
%
%Finally, we present a novel RNN-based model architecture.  The model is small enough to make fast gesture predictions in embedded hardware, and uses appropriate size time windows. We also highlight the power of deep transfer learning by making this model more generalizable. This represents the first recurrent model with such capabilities. In the following sections, we discuss EMG signal processing, filtering, feature extraction, augmentation, and modeling procedures, providing a proper background and precedent for our research.
\section{EMG and sEMG signals}

* Discuss EMG signals, converge to sEMG

\subsection{Challenges}

\subsection{Preprocessing}

* Butterworth filters

* moving average and other fast transforms on rectified data


\subsection{Augmentation}

* electrode shifting

* SNR spectrum sampling


\section{Deep Learning for sEMG Signal}

All discussed in context of sEMG and EMG (obviously discuss generalization between the two)
 \subsection{Recurrent Neural Networks}
 In this research we utilize a special class of neural network known as a \textit{recurrent neural network} (RNN). Before we discuss a recurrent neural network, let us first formalize our definition of a standard, feedforward deep neural network (DNN). A DNN is a network of neurons (small learners which learn simple, nonlinear functions), which are composed and trained in a way such that they learn a function which maps one set of data (for example, an image or a time series) to another set of data (for example, a classification label). Formally, this can be expressed as:

 \begin{equation}
 y(x) = g \left(\sum_{i=0}^d W_{k_i} x_{k_i}\right)
 \end{equation}

 This can be read as follows: given data with $d$ dimensions, the neural network sums up the product of a set of \textit{linear weights} ($W$) and the data at dimension $d$. It then multiplies this by a nonlinear function, $g$. This nonlinearity allows the neural network to build incredibly powerful functional representations of the relationship between $x$ and $y$.
 
 Although a DNN can learn powerful relationships, it is somewhat unintuitve for order-dependent data, such as sequences or time series. This is where an RNN becomes incredibly useful. An RNN not only learns an observation-by-observation functional mapping, but can also map sets of observations (for example, windows of 8-channel electromyographic signal) to labels. They can also map sets of observations to new sets of observations, which is especially important in Natural Language Generation and time series forecasting, however that is beyond the scope of this paper. 
 An RNN accomplishes this in the following manner. 
 First, instead of inputting a single observation, observations are fed in in a while loop. 
 This is not  quite enough to learn long term dependencies within the time windows however. 
 In order to do this, during the while loop of $n$ timesteps, the RNN must also record \emph{state}, that is the RNN must have some sort of memory of previous observations. 
 To do this, the RNN learns a functional mapping which is dependent on its own previous states. This can be expressed mathematically as follows. First, let us define the weights which the RNN uses to update its state, $W_s$:

 \begin{equation}\label{eq:ws}
	 W_s = \left(  I - \left(\Delta T\right) A\right)^{-1}
 \end{equation}
  
 Where $I$ is the identity, $\Delta T$ is is the discrete change in time between observations in the time window, and $A$ is a linear, block-circulant matrix. We can then perform a linear transformation on \autoref{eq:ws} to get the weights for the input signal, $W_x$, and bias $\theta$:

 \begin{align}
	 W_x = \left(\Delta T\right) W_s  C \\
	 \theta = \left(\Delta T\right) W_s \phi
 \end{align}
 
 Where C is another block-circulant, linear transformation matrix, and $phi$ is a learned vector of biases for the RNN (similar to the intercept in regression). This allows us to define the state of the function at timestep $n$ as:

 \begin{equation}\label{eq:almost}
	 \vec{s}\left[ n \right] = W_s \vec{s} \left[ n-1 \right] + W_x \vec{x} \left[ n \right] + \vec{\theta_s}
 \end{equation}

 However, this definition of an RNN has an issue: the state term ($W_s\vec{s}\left[ n-1 \right]$) will grow or shrink without bound, leading to a highly unstable model which in all likelihood fails to converge. To bound it while preserving all of the information, we perform a nonlinear transformation: the hyperbolic tangent. We define a new term, $\vec{r}$, which represents the recurrent portion of an RNN:

 \begin{align}
	 \vec{r}\left[ n \right] = \tanh \left(\vec{s} \left[ n \right]\right) \\
	 W_r = \left(\Delta T\right) W_s  B \\
 \end{align}

 Where $W_r$ represents the recurrent weights of the model, and B is a block circulant linear transformation matrix. We can then rewrite \autoref{eq:almost} as the classic RNN equation:

 \begin{equation}\label{eq:rnn}
	 \vec{s}\left[ n \right] = W_r \vec{r} \left[ n-1 \right] + W_x \vec{x} \left[ n \right] + \vec{\theta_s}
 \end{equation}

 While the simple recurrent neural network can learn powerful functional mappings of time-dependent data, it suffers from difficulties due to training issues. If a single eigenvalue of $W_r$ lies outside of of the range $\left(0, 1\right)$, the gradient used to train the RNN will either explode or decay exponentially, causing the RNN to stop learning \cite{rnn_fun}.

In order to combat this, the Long Short Term Memory (LSTM) is introduced. The LSTM 

 Both of these derivations are based off of the comprehensive work done in \cite{rnn_fun}

 \subsection{Attention}



 \subsection{Deep Transfer Learning}
% Maybe ignore, i dont think its a big contribution of ours, I think it takes away from the rest of what we are doing but that is opinion - David
 \subsection{General evaluation}
% include table of parameters of recent work, highlighting window size, number of layers and cells total
\section{Training Procedures}
%Discuss model training procedures, preprocessing procedures, aug procedures, evaluation procedure in great detail, potentially with a great diagram explaining it all
 \section{Novel RNN based model}
% * Talk about architecure
% * LSTM vs GRU vs RNN appropriateness, number of parameters
 \section{results}
 \subsection{Novel RNN}
% Figure of all the different accuracies with and without transfer learning, discussion of power of results (anything over 70\% accuracy with intra-subject evaluation on the 17 class subset of nina is an excellent result in general, combined with practical utility even better, ignore what i said about transfer learning, all the other papers, we are highlighted in bold
\section{Conclusion} 
%Note that subject splitting may not be of utmost exigence until we can fit within subjects.

 \bibliographystyle{splncs04}
 \bibliography{samplebib}
%
%% End the document
\end{document}
%
